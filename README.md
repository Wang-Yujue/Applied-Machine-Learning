# Applied-Machine-Learning-in-Python
## Fundamentals of Machine Learning - Intro to SciKit Learn
### This module introduces basic machine learning concepts, tasks, and workflow using an example classification problem based on the K-nearest neighbors method, and implemented using the scikit-learn library.
### For the task, using the Breast Cancer Wisconsin (Diagnostic) Database to create a classifier that can help diagnose patients.
## Supervised Machine Learning - Part 1
### This module delves into a wider variety of supervised learning methods for both classification and regression, learning about the connection between model complexity and generalization performance, the importance of proper feature scaling, and how to control model complexity by applying techniques like regularization to avoid overfitting. In addition to k-nearest neighbors, this week covers linear regression (least-squares, ridge, lasso, and polynomial regression), logistic regression, support vector machines, the use of cross-validation for model evaluation, and decision trees.
### In this task, explore the relationship between model complexity and generalization performance, by adjusting key parameters of various supervised learning models. Part 1 of this task will look at regression and Part 2 will look at classification. In Part 2, the data will be used to train a model to predict whether or not a mushroom is poisonous.
## Evaluation
### This module covers evaluation and model selection methods that you can use to help understand and optimize the performance of your machine learning models.
### In this task, train several models and evaluate how effectively they predict instances of fraud using data based on a dataset from Kaggle.
## Supervised Machine Learning - Part 2
### This module covers more advanced supervised learning methods that include ensembles of trees (random forests, gradient boosted trees), and neural networks (with an optional summary on deep learning). You will also learn about the critical problem of data leakage in machine learning and how to detect and avoid it.
### This task is based on a data challenge from the Michigan Data Science Team (MDST).
### The Michigan Data Science Team (MDST) and the Michigan Student Symposium for Interdisciplinary Statistical Sciences (MSSISS) have partnered with the City of Detroit to help solve one of the most pressing problems facing Detroit - blight. Blight violations are issued by the city to individuals who allow their properties to remain in a deteriorated condition. Every year, the city of Detroit issues millions of dollars in fines to residents and every year, many of these fines remain unpaid. Enforcing unpaid blight fines is a costly and tedious process, so the city wants to know: how can we increase blight ticket compliance?

## Module 1: A simple classification task
## Module 2: Supervised Learning, Part I
### K-Nearest Neighbors
#### Classification
#### Regression
#### Regression model complexity as a function of K
### Linear models for regression
#### Linear regression
#### Ridge regression
##### Ridge regression with feature normalization
##### Ridge regression with regularization parameter: alpha
#### Lasso regression
##### Lasso regression with regularization parameter: alpha
#### Polynomial regression
### Linear models for classification
#### Logistic regression
##### Logistic regression for binary classification
##### Logistic regression on simple synthetic dataset
##### Logistic regression regularization: C parameter
##### Application to real dataset
#### Support Vector Machines
#### Multi-class classification with linear models
### Kernelized Support Vector Machines
### Cross-validation
### Validation curve example
### Decision Trees
## Module 3: Evaluation
### Evaluation for Classification
#### Dummy Classifiers
#### Confusion matrices
#### Evaluation metrics for binary classification
#### Decision functions
#### Precision-recall curves
#### ROC curves, Area-Under-Curve (AUC)
#### Evaluation measures for multi-class classification
#### Regression evaluation metrics
#### Model selection using evaluation metrics
#### Two-feature classification example using the digits dataset
## Module 4: Supervised Learning, Part II
### Naive Bayes classifiers
### Ensembles of Decision Trees
### Neural networks
### Unsupervised Learning
### Dimensionality Reduction and Manifold Learning
#### Principal Components Analysis (PCA)
#### Manifold learning methods
### Clustering
#### K-means
#### Agglomerative clustering
#### DBSCAN clustering

