# Applied-Machine-Learning-in-Python
## Module 1: Fundamentals of Machine Learning - Intro to SciKit Learn
### This module introduces basic machine learning concepts, tasks, and workflow using an example classification problem based on the K-nearest neighbors method, and implemented using the scikit-learn library.
## Module 2: Supervised Machine Learning - Part 1
### This module delves into a wider variety of supervised learning methods for both classification and regression, learning about the connection between model complexity and generalization performance, the importance of proper feature scaling, and how to control model complexity by applying techniques like regularization to avoid overfitting. In addition to k-nearest neighbors, this week covers linear regression (least-squares, ridge, lasso, and polynomial regression), logistic regression, support vector machines, the use of cross-validation for model evaluation, and decision trees.
## Module 3: Evaluation
### This module covers evaluation and model selection methods that you can use to help understand and optimize the performance of your machine learning models.
## Module 4: Supervised Machine Learning - Part 2
### This module covers more advanced supervised learning methods that include ensembles of trees (random forests, gradient boosted trees), and neural networks (with an optional summary on deep learning). You will also learn about the critical problem of data leakage in machine learning and how to detect and avoid it.
#### Following topics are included:
## Module 1: A simple classification task
## Module 2: Supervised Learning, Part I
### K-Nearest Neighbors
#### Classification
#### Regression
#### Regression model complexity as a function of K
### Linear models for regression
#### Linear regression
#### Ridge regression
##### Ridge regression with feature normalization
##### Ridge regression with regularization parameter: alpha
#### Lasso regression
##### Lasso regression with regularization parameter: alpha
#### Polynomial regression
### Linear models for classification
#### Logistic regression
##### Logistic regression for binary classification
##### Logistic regression on simple synthetic dataset
##### Logistic regression regularization: C parameter
##### Application to real dataset
#### Support Vector Machines
#### Multi-class classification with linear models
### Kernelized Support Vector Machines
### Cross-validation
### Validation curve example
### Decision Trees
## Module 3: Evaluation
### Evaluation for Classification
#### Dummy Classifiers
#### Confusion matrices
#### Evaluation metrics for binary classification
#### Decision functions
#### Precision-recall curves
#### ROC curves, Area-Under-Curve (AUC)
#### Evaluation measures for multi-class classification
#### Regression evaluation metrics
#### Model selection using evaluation metrics
#### Two-feature classification example using the digits dataset
## Module 4: Supervised Learning, Part II
### Naive Bayes classifiers
### Ensembles of Decision Trees
### Neural networks
### Unsupervised Learning
### Dimensionality Reduction and Manifold Learning
#### Principal Components Analysis (PCA)
#### Manifold learning methods
### Clustering
#### K-means
#### Agglomerative clustering
#### DBSCAN clustering

